{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "780a0c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns\n",
    "from stargazer.stargazer import Stargazer\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "3c52864d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('kickers_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "d78ef53b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Team</th>\n",
       "      <th>Year</th>\n",
       "      <th>GameMinute</th>\n",
       "      <th>Kicker</th>\n",
       "      <th>Distance</th>\n",
       "      <th>ScoreDiff</th>\n",
       "      <th>Grass</th>\n",
       "      <th>Success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PHI</td>\n",
       "      <td>2005</td>\n",
       "      <td>3</td>\n",
       "      <td>Akers</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>PHI</td>\n",
       "      <td>2005</td>\n",
       "      <td>29</td>\n",
       "      <td>Akers</td>\n",
       "      <td>49</td>\n",
       "      <td>-7</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>PHI</td>\n",
       "      <td>2005</td>\n",
       "      <td>51</td>\n",
       "      <td>Akers</td>\n",
       "      <td>44</td>\n",
       "      <td>-7</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>PHI</td>\n",
       "      <td>2005</td>\n",
       "      <td>14</td>\n",
       "      <td>Akers</td>\n",
       "      <td>43</td>\n",
       "      <td>14</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>PHI</td>\n",
       "      <td>2005</td>\n",
       "      <td>60</td>\n",
       "      <td>Akers</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 Team  Year  GameMinute Kicker  Distance  ScoreDiff  Grass  \\\n",
       "0           1  PHI  2005           3  Akers        49          0  False   \n",
       "1           2  PHI  2005          29  Akers        49         -7  False   \n",
       "2           3  PHI  2005          51  Akers        44         -7  False   \n",
       "3           4  PHI  2005          14  Akers        43         14   True   \n",
       "4           5  PHI  2005          60  Akers        23          0   True   \n",
       "\n",
       "   Success  \n",
       "0        0  \n",
       "1        0  \n",
       "2        1  \n",
       "3        0  \n",
       "4        1  "
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "304ad0ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It's important we check for NAN before we start our analysis.\n",
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98907ad4",
   "metadata": {},
   "source": [
    "<h1> PSET 1 Econ 1042 Sports Economics </h1>\n",
    "<h2> 1. Question </h2>\n",
    "<ol>\n",
    "    <li> What was the minimum distance of a field goal kicked in this sample? What was the maximum? Mean? Median!</li>\n",
    "    <li> Why isn’t the minimum lower? (For those who are not familiar with football, please read about how field goal distance is measured and its relationship to where the ball is on the field.)</li>\n",
    "    <li> What special circumstances might explain the maximum? (Hint: football is a game with 4, 15-minute quarters. At the end of the second quarter there is a halftime break and possession is assigned based on the result of a first-half coin toss) </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "6e467f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The median distance of a field goal kicked was 37.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    11187.000000\n",
       "mean        36.897381\n",
       "std         10.173351\n",
       "min         18.000000\n",
       "25%         28.000000\n",
       "50%         37.000000\n",
       "75%         45.000000\n",
       "max         76.000000\n",
       "Name: Distance, dtype: float64"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"The median distance of a field goal kicked was {np.median(data['Distance'])}\")\n",
    "data['Distance'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885af55a",
   "metadata": {},
   "source": [
    "1. The minimum distance of a field goal kicked in the sample was 18.00 yards. The maximum was 76.00 yards and the median was 37.0 yards. The mean was 36.897 yards\n",
    "2. The minimum is 17 yards. This makes sense since the endzone is 10yards, and the ball has to be kicked from 7 yards from the line of scrimmage. Hence 10 + 7 = 17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "2377c96d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Team</th>\n",
       "      <th>Year</th>\n",
       "      <th>GameMinute</th>\n",
       "      <th>Kicker</th>\n",
       "      <th>Distance</th>\n",
       "      <th>ScoreDiff</th>\n",
       "      <th>Grass</th>\n",
       "      <th>Success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3557</th>\n",
       "      <td>3558</td>\n",
       "      <td>OAK</td>\n",
       "      <td>2008</td>\n",
       "      <td>30</td>\n",
       "      <td>Janikowski</td>\n",
       "      <td>76</td>\n",
       "      <td>15</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 Team  Year  GameMinute      Kicker  Distance  ScoreDiff  \\\n",
       "3557        3558  OAK  2008          30  Janikowski        76         15   \n",
       "\n",
       "      Grass  Success  \n",
       "3557   True        0  "
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets find out what play was kicked from 76 yards away?\n",
    "max_yard = data.loc[data['Distance'] == 76.00]\n",
    "max_yard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b082f1c",
   "metadata": {},
   "source": [
    "3. The 76 yards field goal attempt from Janikowski was in the last second of the second quarter (video: https://www.youtube.com/watch?v=X7BepDe6Zoc). It makes sense to kick if far into the opponents end zone, if in the first half your team had the ball. Since, then the opposing team will start from further away from the kickers endzone. It's like as if the special team does a punt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671e0d2d",
   "metadata": {},
   "source": [
    "<h2> 2. Question </h2>\n",
    "<p> Over the entire sample what percentage of kicks from 40 to 45 yards were made? Kicks over 45 yards? <p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "3ddc13c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.844% of Kicks were from between 40-45 yards\n"
     ]
    }
   ],
   "source": [
    "sample_size = len(data)\n",
    "# let's find the number of successful kicks from 40-45 yards\n",
    "kicks_40_45 = data.loc[(data['Distance'] > 40) & (data['Distance'] < 45)]\n",
    "# We find that 1325 kicks were made in that range\n",
    "success_40_45 = kicks_40_45['Success'].value_counts()\n",
    "# print(success_40_45)\n",
    "ratio_success = success_40_45[1]/(success_40_45[0] + success_40_45[1])\n",
    "ratio = (len(kicks_40_45)/sample_size) * 100\n",
    "print(f'{ratio:.3f}% of Kicks were from between 40-45 yards')\n",
    "\n",
    "# How many kicks were over 45 yards?\n",
    "# kicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "6aa068c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.439% of Kicks were from between 40-45 yards\n"
     ]
    }
   ],
   "source": [
    "kicks_above_45 = data.loc[data['Distance'] > 45]\n",
    "ratio_above_45 = (len(kicks_above_45)/sample_size) * 100\n",
    "print(f'{ratio_above_45:.3f}% of Kicks were from between 40-45 yards')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9988943e",
   "metadata": {},
   "source": [
    "<h2> 3. Question </h2>\n",
    "<p> Was the make rate higher on grass or on turf? Is that difference statistically significant? Do you think this is the true effect of surface? Why or why not?  (Answer this by doing an OLS regression. For the entire assignment, let’s use the heteroskedasticity robust standard errors, r in stata or the equivalent in R)<br> <br>\n",
    "Let's compute the difference using $\\Delta = \\bar{Y}_{grass} - \\bar{Y}_{turf}$ we shall report standard errors as heteroscedasticity robust (HC2) \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "dd985180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                Success   R-squared:                       0.001\n",
      "Model:                            OLS   Adj. R-squared:                  0.001\n",
      "Method:                 Least Squares   F-statistic:                     7.500\n",
      "Date:                Wed, 01 Feb 2023   Prob (F-statistic):            0.00618\n",
      "Time:                        21:07:04   Log-Likelihood:                -4845.9\n",
      "No. Observations:               11187   AIC:                             9696.\n",
      "Df Residuals:                   11185   BIC:                             9710.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:                  HC2                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Grass         -0.0193      0.007     -2.739      0.006      -0.033      -0.005\n",
      "Intercept      0.8433      0.005    164.864      0.000       0.833       0.853\n",
      "==============================================================================\n",
      "Omnibus:                     3155.584   Durbin-Watson:                   1.995\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             6555.427\n",
      "Skew:                          -1.781   Prob(JB):                         0.00\n",
      "Kurtosis:                       4.175   Cond. No.                         2.75\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC2)\n"
     ]
    }
   ],
   "source": [
    "# define response variable\n",
    "# statsmodel requires us to add a column where each value is 1 in order to compute intercept\n",
    "data['Intercept'] = 1\n",
    "# Since we find no NAN in our column\n",
    "print(data['Grass'].isnull().values.any())\n",
    "# We can conver the 'bool' values for Grass to the datatype 'int'\n",
    "data['Grass'] = data['Grass'].astype(int)\n",
    "Y = data[['Success']]\n",
    "X = data[['Grass', 'Intercept']]\n",
    "mod = sm.OLS(Y, X)\n",
    "res = mod.fit(cov_type='HC2')\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "1bb9edef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.019329247569502543\n"
     ]
    }
   ],
   "source": [
    "# Let's also try the Sklearn Library\n",
    "# We can also use the Sklearn Library to do an OLS regression, but I don't think it has a summary function.\n",
    "X = data[['Grass']]\n",
    "reg = LinearRegression(fit_intercept=True).fit(X,Y)\n",
    "parameters = reg.get_params()\n",
    "print(reg.coef_[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4878e3",
   "metadata": {},
   "source": [
    "We find that the observed difference is statistically insignificant at the $\\alpha = 0.05$ level. It seems as if the surface does not have an impact on the observed average success rates of field goal kicks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171f63be",
   "metadata": {},
   "source": [
    "<h2> 4. Question </h2>\n",
    "<ol>\n",
    "    <li>\tHow is distance of attempt correlated with surface? What might explain this? (Coaches get to choose when to kick a field goal, one is never forced) </li>\n",
    "    <li> \tHow is distance correlated with make percentage? </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "5f886e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.002551996001227438\n",
      "-0.33693399701495164\n"
     ]
    }
   ],
   "source": [
    "# Let's calculate the correlation between the columns\n",
    "corr_surface = data['Distance'].corr(data['Grass'])\n",
    "corr_success = data['Distance'].corr(data['Success'])\n",
    "print(corr_surface)\n",
    "print(corr_success)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cac743f",
   "metadata": {},
   "source": [
    "The correlation coefficient for distance and or Grass is -0.0025, basically negligible. The correlation coefficiecnt for distance and success rates is -0.3369, meaning as distance increases the success rate goes down. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f82c10",
   "metadata": {},
   "source": [
    "<h2> 5. Question </h2>\n",
    "<ol>\n",
    "    <li>What is the formula for omitted variable bias?</li>\n",
    "    <li>Given (a) what should happen to the estimate of the effect of a kick being on grass when you add in distance? Verify this is true.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae29347",
   "metadata": {},
   "source": [
    "1. Ommitted variable bias arises when the regressor X is correlated with an omitted variable.\n",
    "The formula for omitted variable bias is <br>$\\hat{\\beta}_{1,OLS}= \\frac{Cov(y,x_1)}{Var(x_1)}$\n",
    "2. Since distance and grass are negatively correlated as well as distance and success rate our estimate from the short regression will be positively biased. Ultimately, revealing that kicking on grass is harder then on turf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "06b38d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                Success   R-squared:                       0.114\n",
      "Model:                            OLS   Adj. R-squared:                  0.114\n",
      "Method:                 Least Squares   F-statistic:                     695.7\n",
      "Date:                Wed, 01 Feb 2023   Prob (F-statistic):          1.74e-285\n",
      "Time:                        21:07:04   Log-Likelihood:                -4171.1\n",
      "No. Observations:               11187   AIC:                             8348.\n",
      "Df Residuals:                   11184   BIC:                             8370.\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:                  HC2                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Grass         -0.0200      0.007     -3.004      0.003      -0.033      -0.007\n",
      "Distance      -0.0124      0.000    -37.218      0.000      -0.013      -0.012\n",
      "Intercept      1.2999      0.011    115.413      0.000       1.278       1.322\n",
      "==============================================================================\n",
      "Omnibus:                     2395.590   Durbin-Watson:                   1.982\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             4232.201\n",
      "Skew:                          -1.449   Prob(JB):                         0.00\n",
      "Kurtosis:                       3.824   Cond. No.                         152.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC2)\n"
     ]
    }
   ],
   "source": [
    "Y = data[['Success']]\n",
    "X = data[['Grass', 'Distance', 'Intercept']]\n",
    "mod = sm.OLS(Y, X)\n",
    "res = mod.fit(cov_type='HC2')\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "bcbafa82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                Success   R-squared:                       0.114\n",
      "Model:                            OLS   Adj. R-squared:                  0.114\n",
      "Method:                 Least Squares   F-statistic:                     695.7\n",
      "Date:                Wed, 01 Feb 2023   Prob (F-statistic):          1.74e-285\n",
      "Time:                        21:07:04   Log-Likelihood:                -4171.1\n",
      "No. Observations:               11187   AIC:                             8348.\n",
      "Df Residuals:                   11184   BIC:                             8370.\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:                  HC2                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      1.2999      0.011    115.413      0.000       1.278       1.322\n",
      "Grass         -0.0200      0.007     -3.004      0.003      -0.033      -0.007\n",
      "Distance      -0.0124      0.000    -37.218      0.000      -0.013      -0.012\n",
      "==============================================================================\n",
      "Omnibus:                     2395.590   Durbin-Watson:                   1.982\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             4232.201\n",
      "Skew:                          -1.449   Prob(JB):                         0.00\n",
      "Kurtosis:                       3.824   Cond. No.                         152.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC2)\n"
     ]
    }
   ],
   "source": [
    "mod = smf.ols(formula='Success ~ Grass + Distance', data=data).fit(cov_type='HC2')\n",
    "print(mod.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "da29a21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Team', 'Year', 'GameMinute', 'Kicker', 'Distance',\n",
       "       'ScoreDiff', 'Grass', 'Success', 'Intercept'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d807301",
   "metadata": {},
   "source": [
    "<h2> 6. Question </h2>\n",
    "<ol>\n",
    "    <li> Run an ols regression of kick success on distance, surface, point differential, and clock time. Interpret the coefficients. Does it seem like kickers do better or worse late in the game? Does the score of the game seem to effect them?</li>\n",
    "\n",
    "<li>Now add in kicker fixed effects (i.kicker in Stata), what do these correct for? How does adjusted r-squared change? </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "ee316d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                Success   R-squared:                       0.114\n",
      "Model:                            OLS   Adj. R-squared:                  0.114\n",
      "Method:                 Least Squares   F-statistic:                     347.8\n",
      "Date:                Wed, 01 Feb 2023   Prob (F-statistic):          1.07e-282\n",
      "Time:                        21:07:04   Log-Likelihood:                -4171.0\n",
      "No. Observations:               11187   AIC:                             8352.\n",
      "Df Residuals:                   11182   BIC:                             8389.\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:                  HC2                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      1.2987      0.013    101.462      0.000       1.274       1.324\n",
      "Distance      -0.0124      0.000    -37.158      0.000      -0.013      -0.012\n",
      "Grass         -0.0200      0.007     -3.004      0.003      -0.033      -0.007\n",
      "ScoreDiff     -0.0001      0.000     -0.285      0.776      -0.001       0.001\n",
      "GameMinute  4.408e-05      0.000      0.221      0.825      -0.000       0.000\n",
      "==============================================================================\n",
      "Omnibus:                     2395.605   Durbin-Watson:                   1.981\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             4232.237\n",
      "Skew:                          -1.449   Prob(JB):                         0.00\n",
      "Kurtosis:                       3.824   Cond. No.                         225.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC2)\n"
     ]
    }
   ],
   "source": [
    "mod = smf.ols(formula='Success ~ Distance + Grass + ScoreDiff + GameMinute', data=data).fit(cov_type='HC2')\n",
    "print(mod.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335a421b",
   "metadata": {},
   "source": [
    "<p>1. The Game minute seems to have no statistically significant effect on Kicker performance. Furthermore, the point differential of the game does not seem to be statistically significant either, and also not affect kicker success rates.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "fcb958b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                Success   R-squared:                       0.127\n",
      "Model:                            OLS   Adj. R-squared:                  0.121\n",
      "Method:                 Least Squares   F-statistic:                 8.581e+10\n",
      "Date:                Wed, 01 Feb 2023   Prob (F-statistic):               0.00\n",
      "Time:                        21:07:05   Log-Likelihood:                -4088.0\n",
      "No. Observations:               11187   AIC:                             8350.\n",
      "Df Residuals:                   11100   BIC:                             8987.\n",
      "Df Model:                          86                                         \n",
      "Covariance Type:                  HC2                                         \n",
      "===========================================================================================\n",
      "                              coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------\n",
      "Intercept                   1.2659      0.024     53.087      0.000       1.219       1.313\n",
      "C(Kicker)[T.Andersen]       0.0438      0.047      0.937      0.349      -0.048       0.136\n",
      "C(Kicker)[T.Andrus]        -0.3664      0.216     -1.693      0.090      -0.791       0.058\n",
      "C(Kicker)[T.Bailey]         0.1116      0.031      3.578      0.000       0.050       0.173\n",
      "C(Kicker)[T.Barth]          0.0775      0.033      2.337      0.019       0.013       0.142\n",
      "C(Kicker)[T.Bironas]        0.0760      0.029      2.668      0.008       0.020       0.132\n",
      "C(Kicker)[T.Boswell]        0.1248      0.046      2.693      0.007       0.034       0.216\n",
      "C(Kicker)[T.Brien]         -0.4597      0.275     -1.674      0.094      -0.998       0.079\n",
      "C(Kicker)[T.Brindza]       -0.2372      0.153     -1.553      0.121      -0.537       0.062\n",
      "C(Kicker)[T.Brown]          0.0311      0.026      1.188      0.235      -0.020       0.082\n",
      "C(Kicker)[T.Bryant]         0.0572      0.028      2.079      0.038       0.003       0.111\n",
      "C(Kicker)[T.Buehler]       -0.0320      0.078     -0.411      0.681      -0.184       0.120\n",
      "C(Kicker)[T.Bullock]        0.0350      0.044      0.801      0.423      -0.051       0.121\n",
      "C(Kicker)[T.Carney]         0.0037      0.035      0.105      0.916      -0.065       0.073\n",
      "C(Kicker)[T.Carpenter]      0.0690      0.029      2.375      0.018       0.012       0.126\n",
      "C(Kicker)[T.Catanzaro]      0.0925      0.040      2.292      0.022       0.013       0.172\n",
      "C(Kicker)[T.Coons]          0.0628      0.055      1.150      0.250      -0.044       0.170\n",
      "C(Kicker)[T.Cortez]        -0.1175      0.108     -1.085      0.278      -0.330       0.095\n",
      "C(Kicker)[T.Coutu]         -0.7064      0.022    -32.777      0.000      -0.749      -0.664\n",
      "C(Kicker)[T.Crosby]         0.0113      0.029      0.392      0.695      -0.045       0.068\n",
      "C(Kicker)[T.Cundiff]       -0.0331      0.035     -0.943      0.346      -0.102       0.036\n",
      "C(Kicker)[T.Dawson]         0.0607      0.027      2.220      0.026       0.007       0.114\n",
      "C(Kicker)[T.Edinger]       -0.0719      0.079     -0.915      0.360      -0.226       0.082\n",
      "C(Kicker)[T.Elam]           0.0350      0.035      1.004      0.315      -0.033       0.103\n",
      "C(Kicker)[T.Elling]        -0.5950      0.023    -25.669      0.000      -0.640      -0.550\n",
      "C(Kicker)[T.Feely]          0.0476      0.029      1.634      0.102      -0.010       0.105\n",
      "C(Kicker)[T.Folk]           0.0112      0.031      0.363      0.717      -0.050       0.072\n",
      "C(Kicker)[T.Forbath]        0.0543      0.045      1.213      0.225      -0.033       0.142\n",
      "C(Kicker)[T.France]        -0.0472      0.134     -0.353      0.724      -0.309       0.215\n",
      "C(Kicker)[T.Franks]         0.0361      0.091      0.398      0.691      -0.142       0.214\n",
      "C(Kicker)[T.Freese]        -0.3562      0.159     -2.245      0.025      -0.667      -0.045\n",
      "C(Kicker)[T.Gano]           0.0173      0.034      0.510      0.610      -0.049       0.084\n",
      "C(Kicker)[T.Gostkowski]     0.0601      0.027      2.238      0.025       0.007       0.113\n",
      "C(Kicker)[T.Gould]          0.0664      0.027      2.448      0.014       0.013       0.120\n",
      "C(Kicker)[T.Graham]         0.0374      0.029      1.311      0.190      -0.019       0.093\n",
      "C(Kicker)[T.Gramatica]      0.0166      0.083      0.200      0.842      -0.147       0.180\n",
      "C(Kicker)[T.Hall]           0.0287      0.071      0.402      0.687      -0.111       0.168\n",
      "C(Kicker)[T.Hanson]         0.0673      0.031      2.181      0.029       0.007       0.128\n",
      "C(Kicker)[T.Hartley]        0.0105      0.041      0.257      0.797      -0.070       0.091\n",
      "C(Kicker)[T.Hauschka]       0.0743      0.030      2.445      0.014       0.015       0.134\n",
      "C(Kicker)[T.Henery]         0.0183      0.042      0.440      0.660      -0.063       0.100\n",
      "C(Kicker)[T.Hocker]        -0.0916      0.118     -0.775      0.438      -0.323       0.140\n",
      "C(Kicker)[T.Hopkins]        0.1042      0.056      1.861      0.063      -0.006       0.214\n",
      "C(Kicker)[T.Janikowski]     0.0514      0.029      1.802      0.072      -0.005       0.107\n",
      "C(Kicker)[T.Kaeding]        0.0432      0.031      1.396      0.163      -0.017       0.104\n",
      "C(Kicker)[T.Kasay]          0.0653      0.030      2.186      0.029       0.007       0.124\n",
      "C(Kicker)[T.Koenen]        -0.4115      0.137     -3.005      0.003      -0.680      -0.143\n",
      "C(Kicker)[T.Lambo]          0.0743      0.067      1.101      0.271      -0.058       0.207\n",
      "C(Kicker)[T.Lindell]        0.0325      0.030      1.086      0.277      -0.026       0.091\n",
      "C(Kicker)[T.Longwell]       0.0408      0.032      1.268      0.205      -0.022       0.104\n",
      "C(Kicker)[T.Mare]          -0.0100      0.033     -0.301      0.763      -0.075       0.055\n",
      "C(Kicker)[T.McManus]        0.0631      0.049      1.295      0.195      -0.032       0.159\n",
      "C(Kicker)[T.Medlock]       -0.1423      0.131     -1.083      0.279      -0.400       0.115\n",
      "C(Kicker)[T.Mehlhaff]      -0.1027      0.264     -0.388      0.698      -0.621       0.416\n",
      "C(Kicker)[T.Murray]         0.1003      0.086      1.165      0.244      -0.068       0.269\n",
      "C(Kicker)[T.Myers]          0.0621      0.066      0.946      0.344      -0.067       0.191\n",
      "C(Kicker)[T.Nedney]         0.0743      0.033      2.231      0.026       0.009       0.140\n",
      "C(Kicker)[T.Novak]          0.0325      0.034      0.963      0.336      -0.034       0.099\n",
      "C(Kicker)[T.Nugent]         0.0083      0.030      0.275      0.783      -0.051       0.068\n",
      "C(Kicker)[T.Parkey]         0.0666      0.055      1.205      0.228      -0.042       0.175\n",
      "C(Kicker)[T.Peterson]       0.0538      0.057      0.943      0.345      -0.058       0.165\n",
      "C(Kicker)[T.Pettrey]       -0.4034      0.253     -1.597      0.110      -0.899       0.092\n",
      "C(Kicker)[T.Potter]        -0.0665      0.203     -0.328      0.743      -0.465       0.331\n",
      "C(Kicker)[T.Prater]         0.0531      0.031      1.718      0.086      -0.007       0.114\n",
      "C(Kicker)[T.Rackers]        0.0531      0.029      1.801      0.072      -0.005       0.111\n",
      "C(Kicker)[T.Rayner]        -0.0642      0.050     -1.293      0.196      -0.161       0.033\n",
      "C(Kicker)[T.Reed]           0.0236      0.032      0.731      0.465      -0.040       0.087\n",
      "C(Kicker)[T.Santos]         0.0574      0.045      1.283      0.200      -0.030       0.145\n",
      "C(Kicker)[T.Schmitt]       -0.1605      0.368     -0.436      0.663      -0.881       0.560\n",
      "C(Kicker)[T.Scifres]        0.2547      0.021     12.298      0.000       0.214       0.295\n",
      "C(Kicker)[T.Scobee]         0.0329      0.030      1.088      0.277      -0.026       0.092\n",
      "C(Kicker)[T.Stitser]       -0.0093      0.135     -0.068      0.946      -0.275       0.256\n",
      "C(Kicker)[T.Stover]         0.0453      0.032      1.403      0.161      -0.018       0.109\n",
      "C(Kicker)[T.Sturgis]        0.0080      0.044      0.180      0.857      -0.079       0.095\n",
      "C(Kicker)[T.Succop]         0.0420      0.033      1.272      0.203      -0.023       0.107\n",
      "C(Kicker)[T.Suisham]        0.0298      0.030      1.005      0.315      -0.028       0.088\n",
      "C(Kicker)[T.Tucker]         0.1047      0.031      3.350      0.001       0.043       0.166\n",
      "C(Kicker)[T.Tynes]         -0.0130      0.032     -0.408      0.683      -0.075       0.049\n",
      "C(Kicker)[T.Vanderjagt]    -0.0203      0.058     -0.349      0.727      -0.134       0.094\n",
      "C(Kicker)[T.Vinatieri]      0.0619      0.027      2.275      0.023       0.009       0.115\n",
      "C(Kicker)[T.Walsh]          0.0711      0.035      2.031      0.042       0.002       0.140\n",
      "C(Kicker)[T.Wilkins]        0.0438      0.042      1.043      0.297      -0.039       0.126\n",
      "C(Kicker)[T.Zuerlein]       0.0277      0.041      0.681      0.496      -0.052       0.107\n",
      "Distance                   -0.0125      0.000    -37.174      0.000      -0.013      -0.012\n",
      "Grass                      -0.0246      0.007     -3.314      0.001      -0.039      -0.010\n",
      "ScoreDiff               -3.034e-05      0.000     -0.083      0.934      -0.001       0.001\n",
      "GameMinute               4.117e-05      0.000      0.207      0.836      -0.000       0.000\n",
      "==============================================================================\n",
      "Omnibus:                     2348.948   Durbin-Watson:                   2.007\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             4107.577\n",
      "Skew:                          -1.426   Prob(JB):                         0.00\n",
      "Kurtosis:                       3.827   Cond. No.                     5.45e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC2)\n",
      "[2] The condition number is large, 5.45e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/deep/lib/python3.10/site-packages/statsmodels/base/model.py:1871: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 86, but rank is 85\n",
      "  warnings.warn('covariance of constraints does not have full '\n"
     ]
    }
   ],
   "source": [
    "mod = smf.ols(formula='Success ~ Distance + Grass + ScoreDiff + GameMinute + C(Kicker)', data=data).fit(cov_type='HC2')\n",
    "print(mod.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7bcc99",
   "metadata": {},
   "source": [
    "<p> 2. Adjusted R-square goes up, since we adjusting our regression for individuals attributes that do not vary over the time of a season."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685ebdb9",
   "metadata": {},
   "source": [
    "<h2> 7. Question </h2>\n",
    "<ol>\n",
    "    <li> After you run the regression in part 6, do the command to predict fitted values from this regression: “predict, xb” in stata and equivalent in R “predict.lm”. Based on this, what would you predict the probability of Justin Tucker cutting the lead to 8 (scorediff was -11) in 2015, when the gameminute was 30, and he was on turf</li>\n",
    "    <ol>\n",
    "        <li> We can't build a model using all inputs, and then try to predict sth when missing one value. Missing here is an input for the <strong>Distance.</strong> Hence we must find it ourselves in the data.</li>\n",
    "    </ol>\n",
    "\n",
    "<li>Does this estimate strike you as reasonable?</li>\n",
    "<li>What would the estimate be for an average kicker?</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "9cc9e1cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93507468])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tucker = data.loc[data['Kicker'] == 'Tucker']\n",
    "X = data_tucker[['Intercept', 'Distance', 'Grass', 'ScoreDiff', 'GameMinute']]\n",
    "Y = data_tucker[['Success']]\n",
    "reg_ols_tucker = sm.OLS(Y,X).fit(cov_type='HC2')\n",
    "reg_ols_tucker.predict([1,30, False, -11, 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "e475672d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Linear_Tucker', -0.012015139013776488, 0.09939115007000607, 0.0008766873127663746, 0.0007538568196675142, 0.9350746758606505]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/deep/lib/python3.10/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Grass</th>\n",
       "      <th>ScoreDiff</th>\n",
       "      <th>GameMinute</th>\n",
       "      <th>Proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear_Tucker</td>\n",
       "      <td>-0.012015</td>\n",
       "      <td>0.099391</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>0.935075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model  Distance     Grass  ScoreDiff  GameMinute     Proba\n",
       "0  Linear_Tucker -0.012015  0.099391   0.000877    0.000754  0.935075"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's use the Sklearn library for this prediction. Since we can specify the input.\n",
    "# Since we are interested in Tucker\n",
    "data_tucker = data.loc[data['Kicker'] == 'Tucker']\n",
    "X = data_tucker[['Distance', 'Grass', 'ScoreDiff', 'GameMinute']]\n",
    "Y = data_tucker[['Success']]\n",
    "reg = LinearRegression(fit_intercept=True).fit(X,Y)\n",
    "parameters = reg.get_params()\n",
    "# Let's find the distance of the kick from tucker with scroediff -11, gameminute 30, and on turf\n",
    "tucker = data_tucker.loc[(data_tucker['ScoreDiff']==-11)&(data_tucker['GameMinute']==30) & (data_tucker['Grass']==False)]\n",
    "tucker\n",
    "\n",
    "reg_coef = list(np.ravel(reg.coef_))\n",
    "reg_coef.insert(0, 'Linear_Tucker')\n",
    "proba = reg.predict([[30, False, -11, 30]])[0][0]\n",
    "reg_coef.append(proba)\n",
    "results = pd.DataFrame([reg_coef], columns=['Model', 'Distance', 'Grass', 'ScoreDiff', 'GameMinute', 'Proba'])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "5dfa1c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9350746758606505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/deep/lib/python3.10/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# This is the play that the question is asking for\n",
    "# We can ignore the warning message\n",
    "tucker = [[30, False, -11, 30]]\n",
    "print(reg.predict(tucker)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5023ef4",
   "metadata": {},
   "source": [
    "2. Let's see if this estimate seems reasonable for Tucker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "4f99f6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    61\n",
       "Name: Success, dtype: int64"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tucker_35_yards = data_tucker.loc[(data_tucker['Distance'] < 35)]\n",
    "tucker_35_yards['Success'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779d7e09",
   "metadata": {},
   "source": [
    "In the sample Tucker made $61$ kicks that were below 35 yards and he made every single one of them. Hence, the estimate in 1.) seems reasonable. On average in the sample we have $4765$ observations of Kicks that were shot at less then 35 yard distance. Of those $0.947\\%$ were successful, hence the predicted probability seems to be a reasonable estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "85054ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    4513\n",
       "0     252\n",
       "Name: Success, dtype: int64"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_35_yards = data.loc[data['Distance'] < 35]\n",
    "data_35_yards['Success'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace9ee9e",
   "metadata": {},
   "source": [
    "3. Let's predict the probability for an average kicker for that same shot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "7dfea5db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93001065])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the Statsmodels library\n",
    "X = data[['Intercept', 'Distance', 'Grass', 'ScoreDiff', 'GameMinute']]\n",
    "Y = data[['Success']]\n",
    "reg_ols = sm.OLS(Y,X).fit(cov_type='HC2')\n",
    "reg_ols.predict([1,30, False, -11, 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "23a90f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/deep/lib/python3.10/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Grass</th>\n",
       "      <th>ScoreDiff</th>\n",
       "      <th>GameMinute</th>\n",
       "      <th>Proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear</td>\n",
       "      <td>-0.012372</td>\n",
       "      <td>-0.019972</td>\n",
       "      <td>-0.000102</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.930011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model  Distance     Grass  ScoreDiff  GameMinute     Proba\n",
       "0  Linear -0.012372 -0.019972  -0.000102    0.000044  0.930011"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data[['Distance', 'Grass', 'ScoreDiff', 'GameMinute']]\n",
    "Y = data[['Success']]\n",
    "reg = LinearRegression(fit_intercept=True).fit(X,Y)\n",
    "parameters = reg.get_params()\n",
    "reg_coef = list(np.ravel(reg.coef_))\n",
    "reg_coef.insert(0, 'Linear')\n",
    "proba = reg.predict([[30, False, -11, 30]])[0][0]\n",
    "reg_coef.append(proba)\n",
    "results = pd.DataFrame([reg_coef], columns=['Model', 'Distance', 'Grass', 'ScoreDiff', 'GameMinute', 'Proba'])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f9f84a",
   "metadata": {},
   "source": [
    "We estimate the probability for an average kicker to be 0.93%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2c322b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h2> 8. Question </h2>\n",
    "<ol>\n",
    "    <li>Now run a logistic regression with the same specification as in question 6. Use the command predict. Now what is the predicted probability of Tucker making that field goal? (the predict command in stata is now just “predict”)</li>\n",
    "    <li>Why do the coefficients look so different for the logistic regression vs. OLS </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "0b510287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We find the prediction for success to be 0.9400478885815359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/deep/lib/python3.10/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Distance</th>\n",
       "      <th>Grass</th>\n",
       "      <th>ScoreDiff</th>\n",
       "      <th>GameMinute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.102842</td>\n",
       "      <td>-0.167622</td>\n",
       "      <td>-0.000962</td>\n",
       "      <td>0.000384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Distance     Grass  ScoreDiff  GameMinute\n",
       "0 -0.102842 -0.167622  -0.000962    0.000384"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data[['Distance', 'Grass', 'ScoreDiff', 'GameMinute']]\n",
    "Y = data['Success']\n",
    "model_log = LogisticRegression(fit_intercept=True).fit(X,Y)\n",
    "parameters = model_log.get_params()\n",
    "pred = model_log.predict_proba([[30, False, -11, 30]])\n",
    "model_log_coef = [list(np.ravel(model_log.coef_))]\n",
    "print(f'We find the prediction for success to be {pred[0][1]}')\n",
    "coef = pd.DataFrame(model_log_coef, columns=['Distance', 'Grass', 'ScoreDiff', 'GameMinute'])\n",
    "coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d086f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# let's make it a bit prettier using the Stagazer library and the statsmodels package\n",
    "X = data[['Intercept', 'Distance', 'Grass', 'ScoreDiff', 'GameMinute']]\n",
    "log = sm.Logit(Y,X).fit()\n",
    "pred = log.predict([1,30,False, -11, 30])\n",
    "# print(log.predict([1, 30, False, -11, 30]))\n",
    "# print(log.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99986f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [reg_ols, log]\n",
    "table = Stargazer(models)\n",
    "table.custom_columns(['OLS Model', 'Logit Model'], [1, 1])\n",
    "table.covariate_order(['Intercept', 'Distance', 'Grass', 'ScoreDiff', 'GameMinute'])\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2616b8ee",
   "metadata": {},
   "source": [
    "2. Why do the coefficients look so different for the logistic regression vs. OLS <br>\n",
    "\n",
    "Inherently a logit model and an OLS model are two different models. The logit model uses a sigmoid function, whereas the OLS model uses a simple linear function. Naturally, we end up with different coefficient values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1176079e",
   "metadata": {},
   "source": [
    "<hline>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923d71d2",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h2> 9. Question </h2>\n",
    "<ol>\n",
    "    <li> Who would you say was the best kicker in the NFL over this period? Why? Define best in at least two different ways. Try to quantify the size of the difference. </li>\n",
    "    <li> Are these differences stable over time? For example, if players switch team or year over year? </li>\n",
    "</ol>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f79688",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['Kicker'] == 'Elling']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403b6b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do we define best kicker? A good kicker is one that makes most of the kicks? \n",
    "# Let's try the approach where the best kicker is the most accurate e.g makes most of their Kicks regardless of distance\n",
    "\n",
    "kicker_list = data['Kicker'].unique()\n",
    "\n",
    "def data_per_kicker(kickers):\n",
    "    accuracies = []\n",
    "    for i in kickers:\n",
    "        data_kicker = data.loc[data['Kicker'] == i]\n",
    "        success = data_kicker['Success'].value_counts()\n",
    "        try:\n",
    "            accuracy = success[1]/(success[1] + success[0])\n",
    "            accuracies.append({'Kicker' : i, 'Shots_taken': (success[1] + success[0]), 'Success': success[1], 'Accuracy' : accuracy})\n",
    "        except KeyError:\n",
    "            accuracies.append({'Kicker': i, 'Accuracy' : np.nan})\n",
    "    return accuracies\n",
    "            \n",
    "results = data_per_kicker(kicker_list)\n",
    "frame = pd.DataFrame(results)\n",
    "frame.sort_values(ascending=False, by='Accuracy')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9f81fa",
   "metadata": {},
   "source": [
    "According to this metric in our Sample Boswell was the most accurate Kicker. Then again we shall not forget that this is merely an average of all kicks, hence the value is hugely influenced, by how many kicks the respective Kicker did. Kickers such as Elling, Coutu, and Scifres had not 1 successful kick in our sample. <br>\n",
    "\n",
    "Let's try and quantify the best kicker in a new way. Let's look at accuracy rate when the $-4 < Score < 0$ , meaning that the field goal could mean the change in winning vs losing team vs draw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657504b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the same function as defined before.\n",
    "kicker_list = data['Kicker'].unique()\n",
    "\n",
    "def data_per_kicker(kickers):\n",
    "    accuracies = []\n",
    "    cond = (data['ScoreDiff']<0) & (data['ScoreDiff'] > - 4)\n",
    "    for i in kickers:\n",
    "        data_kicker = data.loc[(data['Kicker'] == i)]\n",
    "        # For each kicker we only look at the shots they take that \n",
    "        data_kicker = data_kicker.loc[(data_kicker['ScoreDiff'] < 0) & (data_kicker['ScoreDiff'] > - 4)]\n",
    "        success = data_kicker['Success'].value_counts()\n",
    "        try:\n",
    "            accuracy = success[1]/(success[1] + success[0])\n",
    "            accuracies.append({'Kicker' : i, 'Shots_taken_ScoreDiff': (success[1] + success[0]), 'Success_ScoreDiff': success[1], 'Accuracy_ScoreDiff' : accuracy})\n",
    "        except KeyError:\n",
    "            accuracies.append({'Kicker': i, 'Accuracy_ScoreDiff' : np.nan})\n",
    "    return accuracies\n",
    "\n",
    "results = data_per_kicker(kicker_list)\n",
    "frame_2 = pd.DataFrame(results)\n",
    "\n",
    "df = frame.merge(frame_2, on='Kicker')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cbaaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now sort and try to find the Kicker who makes most of the shots when it matters the most\n",
    "print(df['Accuracy_ScoreDiff'].isna().sum())\n",
    "df.sort_values(ascending=False, by='Accuracy_ScoreDiff')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f196ab93",
   "metadata": {},
   "source": [
    "We find that Longwell made 15 out of 16 shots, that turned the game. We also find that there are 23 Kickers in our sample that did not make a Kick when there score difference was between -3 and 0. We also find that Boswell who was our previous best Kicker now to not be in the rankings since he didnt take a single kick when the Score was between -3 and 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c93525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Kicker'] == 'Boswell']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471ca8a7",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3> Let's look at trends over time and more so if performance of kickers is persistent over time.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966d971e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It makes sense to look at the distribution of observations per year to understand if we have an evenly distributed sample.\n",
    "sns.kdeplot(data=data, x='Year')\n",
    "plt.vlines(2005, 0 , 0.1, linestyles='dotted', color = 'r')\n",
    "plt.vlines(2015, 0 , 0.1, linestyles='dotted', color = 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc0885c",
   "metadata": {},
   "source": [
    "It seems as if our sample contains roughly the same yearly amount of kicks between 2005 - 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92d0827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the same function as before but modify it to take into account years\n",
    "# We use the same function as defined before.\n",
    "kicker_list = data['Kicker'].unique()\n",
    "\n",
    "def data_per_kicker(kickers):\n",
    "    accuracies = []\n",
    "    for i in kickers:\n",
    "        # this is the dataframe for each kicker\n",
    "        data_kicker = data.loc[(data['Kicker'] == i)]\n",
    "        # For each kicker we only look at the shots they take that year\n",
    "        years_played = data_kicker['Year'].unique()\n",
    "        year = []\n",
    "        for year in years_played:\n",
    "            # this is the dataframe for each kicker for each year\n",
    "            data_kicker_yearly = data_kicker.loc[(data['Year'] == year)]\n",
    "            success = data_kicker_yearly['Success'].value_counts()\n",
    "            if len(success) != 1:\n",
    "                accuracy = success[1]/(success[1] + success[0])\n",
    "                accuracies.append({'Year': year, 'Kicker' : i, 'Shots_taken': (success[1] + success[0]), 'Success': success[1], 'Accuracy' : accuracy})\n",
    "            else:\n",
    "                continue\n",
    "    return accuracies\n",
    "\n",
    "results = data_per_kicker(kicker_list)\n",
    "frame_years = pd.DataFrame(results)\n",
    "frame_years"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01d8387",
   "metadata": {},
   "source": [
    "Since we can't plot all players. Let's create another table where we identify the players with the highest variance in accuracy(e.g. the least consistent) and those who have the lowest variance over the sample space.\n",
    "\n",
    "For each kicker I calculate the variance $S^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n-1}$ where:\n",
    "<ul>\n",
    "    <li> $x_i$ = the value of the observation </li>\n",
    "    <li> $\\bar{x}$ = the mean value of all observations </li>\n",
    "    <li> $n$ = the number of observations </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71526135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use the var() to do this for us by specifiying the column\n",
    "kickers_names = frame_years['Kicker'].unique()\n",
    "results_var = []\n",
    "for name in kickers_names:\n",
    "    data = frame_years.loc[frame_years['Kicker'] == name]\n",
    "    var = data.var(numeric_only='Float')\n",
    "    results_var.append({'Kicker': name, 'Variance': var['Accuracy']})\n",
    "    \n",
    "results_var = pd.DataFrame(results_var)\n",
    "results_var = results_var.sort_values(ascending=False, by='Variance').dropna()\n",
    "# these are the 5 highest variance players\n",
    "results_var.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4af151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these have the lowest variance\n",
    "results_var.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b78f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's grab the first 5 kickers and last 5 kickers to plot\n",
    "kickers = list(results_var['Kicker'].unique())\n",
    "kickers = kickers[:5] + kickers[-5:] \n",
    "\n",
    "fig, axs = plt.subplots(1,1,figsize=(16,9))\n",
    "\n",
    "for x in kickers:\n",
    "    data = frame_years.loc[frame_years['Kicker'] == x]\n",
    "    plt.plot(data['Year'], data['Accuracy'], label=f'{x}')\n",
    "    plt.xlabel('Years')\n",
    "    plt.ylabel('Accuracy Score')\n",
    "    plt.title('Variation in accuracy score for the 5 most consistent and least conistent players')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64da348",
   "metadata": {},
   "source": [
    "Interestingly, we can observe that some players such as Hauschka improved thei kick accuracy year by year. This concludes question 9 part 2."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55c58638",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h1> Question 10</h1>\n",
    "<p> 10.\tSome argue that kickers get better with experience, in this dataset do you see evidence to support this conjecture? Try both a linear and quadratic specification. (For simplicity assume that there were no kicks attempted before the beginning of the dataset). What might be wrong with your estimates (besides incomplete data)? Explain! <p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d3b58e",
   "metadata": {},
   "source": [
    "We can answer this question by looking at the accuracies over time and whether this increases as experience goes up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98374af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute experience in this dataset for each individual\n",
    "frame_years['Exp'] = 0\n",
    "names = frame_years['Kicker'].unique()\n",
    "for i in names:\n",
    "    frame_years.loc[(frame_years['Kicker'] == i), 'Exp'] = range(0, len(frame_years.loc[frame_years['Kicker'] == i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "77b9a7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               Accuracy   R-squared:                       0.053\n",
      "Model:                            OLS   Adj. R-squared:                  0.051\n",
      "Method:                 Least Squares   F-statistic:                     21.84\n",
      "Date:                Wed, 01 Feb 2023   Prob (F-statistic):           4.08e-06\n",
      "Time:                        21:37:49   Log-Likelihood:                 342.44\n",
      "No. Observations:                 395   AIC:                            -680.9\n",
      "Df Residuals:                     393   BIC:                            -672.9\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:                  HC2                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Exp            0.0087      0.002      4.673      0.000       0.005       0.012\n",
      "Intercept      0.7876      0.010     82.432      0.000       0.769       0.806\n",
      "==============================================================================\n",
      "Omnibus:                      208.795   Durbin-Watson:                   1.792\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1388.476\n",
      "Skew:                          -2.196   Prob(JB):                    3.14e-302\n",
      "Kurtosis:                      11.067   Cond. No.                         6.81\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC2)\n"
     ]
    }
   ],
   "source": [
    "# Linear\n",
    "data = frame_years.copy()\n",
    "data['Intercept'] = 1\n",
    "Y = data[['Accuracy']]\n",
    "X = data[['Exp', 'Intercept']]\n",
    "mod = sm.OLS(Y, X)\n",
    "res = mod.fit(cov_type='HC2')\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "06bd77fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01711115 -0.00103364]\n"
     ]
    }
   ],
   "source": [
    "# Quadratic\n",
    "# We need to use the Sklearn library for this\n",
    "data = frame_years.copy()\n",
    "data['Intercept'] = 1\n",
    "Y = data[['Accuracy']]\n",
    "X = np.array(data[['Exp']])\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly_features = poly.fit_transform(X.reshape(-1, 1))\n",
    "reg = LinearRegression(fit_intercept=True).fit(poly_features, Y)\n",
    "print(reg.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462cdd2d",
   "metadata": {},
   "source": [
    "<strong> Answer: </strong> I find that as experience goes up the accuracy of the players goes up as well. This estimate is rather small but statistically significant. Our estimate could be wrong, since some players might have gotten injured and hence were excluded from the experience category, but still continued learning about their game."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "632b277f",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h1> Question 11. </h1> <br>\n",
    "What are the omitted variables you would want in this dataset? List at least 3 and which direction the bias of excluding them could go and why. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fe2f7d",
   "metadata": {},
   "source": [
    "I think it would be cool to look at factors such as age and also positional data. I know that there are datasets on Kaggle for example in the (NFL big data bowl competition) where movement(X, Y coordinates) as well as speed at relative times are recorded. I also think it would be interesting to see whether Kickers are better when their contracts are about to expire or whether that does not apply to Kickers. Excluding age could be a positive bias since, we could imagine that younger Kickers are more fit than older ones. On the other hand, we might discover that age is statistically insignificant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d502ab79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
